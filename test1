                                                             Q1.) CNN_MNLIST(1)

# Step 1: Import necessary libraries
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
     
# Step 2: Load and preprocess the MNIST dataset
# Load the dataset from Keras
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Reshape the data to add a color channel dimension (since images are grayscale)
train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))
     
# Normalize the pixel values to range from 0 to 1
train_images, test_images = train_images / 255.0, test_images / 255.0
     
# Step 3: Build the CNN model
model = models.Sequential()

# Add a convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))

# Add a max-pooling layer
model.add(layers.MaxPooling2D((2, 2)))
     
# Add a second convolutional layer
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
     
# Add a max-pooling layer
model.add(layers.MaxPooling2D((2, 2)))
     
# Add a third convolutional layer
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
     
# Flatten the output for the dense layers
model.add(layers.Flatten())
     
# Add a fully connected layer (Dense layer)
model.add(layers.Dense(64, activation='relu'))
     
# Add the output layer with 10 neurons (one for each digit) and softmax activation
model.add(layers.Dense(10, activation='softmax'))

# Step 4: Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
     
# Step 5: Train the model
history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)

# Step 6: Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels)

print(f'Test accuracy: {test_acc}')

# Step 7: Visualize the accuracy and loss over epochs
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.9, 1])
plt.legend(loc='lower right')
plt.show()

# Step 8: Make predictions on test data
predictions = model.predict(test_images)

# Visualize the first test image and its predicted label
plt.imshow(test_images[0].reshape(28, 28), cmap=plt.cm.binary)
plt.title(f'Predicted Label: {predictions[0].argmax()}')
plt.show()

                                                               Q2.)Copy_of_DL2

import pandas as pd
from keras.layers import Dense
from keras.models import Sequential
     
from google.colab import files
uploaded = files.upload()

dataSet = pd.read_csv('diabetes.csv')
dataSet.describe()

x= dataSet.drop('Outcome', axis=1)
y = dataSet['Outcome']
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(1, activation='sigmoid'))    

model.fit(x,y, epochs=150, batch_size=10)
accuracy = model.evaluate(x,y)
print("model accuracy is ",accuracy)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
     
prediction = model.predict(x)
prediction

model.summary()

                                                         Q3.) DataAnalytics_Demo

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns
     
from google.colab import files
uploaded = files.upload()

df = pd.read_csv('grapes_new.csv')
# Dataset is now stored in a Pandas Dataframe
df

df.columns

#Separate input output data
x=df.drop('CLASS',axis=1)
y=df['CLASS']
     
x.shape

x.info()

#Multi class classification
y.value_counts()

#2. Data cleaning-Missing values
x.isnull().sum()

x.fillna(method='backfill',inplace=True)
x.fillna(method='pad',inplace=True)
     
x.isnull().sum()

#3. Exploratory data analysis
x.describe()

#Label Encode
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
x['SHAPE']=le.fit_transform(x['SHAPE'])
x['SOIL_TYPE']=le.fit_transform(x['SOIL_TYPE'])

x.describe()

sns.kdeplot(df['COLOR_INTENSITY'])

temp=df[['COLOR_INTENSITY','RIPENESS_PER','ALCOHOL_PER','FLAVANOIDS','FERT_NITRO_PER','CLASS']]

sns.pairplot(temp,hue='CLASS',palette='tab10')

from sklearn.feature_selection import SelectKBest, chi2
     
skf = SelectKBest(score_func = chi2, k=5)
     
x_new=skf.fit_transform(x,y)

x_new.shape

skf.get_support()

x_new=x.iloc[:,skf.get_support()]
     
x_new

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x_scaled=scaler.fit_transform(x_new)
     
pd.DataFrame(x_scaled,columns=x_new.columns).describe()

from sklearn.model_selection import train_test_split
     
x_train,x_test,y_train,y_test=train_test_split(
x_scaled,y,random_state=0)
     
x_train.shape

x_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV

params={
    'random_state':[0,1,2,3,4,5],
    'criterion':['gini','entropy'],
    'splitter':['best','random']
}
     
dt=DecisionTreeClassifier()

grid=GridSearchCV(dt,param_grid=params,cv=5,scoring='accuracy')
     
grid.fit(x_train, y_train)

y_pred =grid.predict(x_test)
     
accuracy_score(y_test,y_pred)

ConfusionMatrixDisplay.from_predictions(y_test,y_pred)

import joblib

                                                   Q4.) Day 2 Data Analysis Using Pandas

import os
os.getcwd()

import pandas as pd

df=pd.read_csv('student.csv')
df

type(df)
df1=pd.read_csv('student1.csv')
df1

df1=pd.read_csv('student1.csv',names=['Rno','Name','Marks','Age'])
df1

df3=pd.read_csv('student3.tsv')
df3

df3=pd.read_csv('student3.tsv',sep='\t')
df3

df.shape
df.ndim
df.columns
df.dtypes
df.T
df.head(3)
df.tail(3)
df.iloc[2:8,1:4]
df.iloc[2:,:4]
df.iloc[2:8,:4]
df.iloc[:,:]
x=[1,2,3]
y=x
x.append(4)
x
y
df.iloc[3,:]
x=df.iloc[:,1]
type(x)
df.iloc[3,1]
df.iloc[[3,6,8,1],:]
df.iloc[3,:]
df.iloc[[3,6,8,1],[1,4]]
df.loc[:,['name','age']]
df.name
df[['name','marks']]
df.iloc[[3],:]
df.iloc[3,:]
df3.drop(5)
df.drop([5,7,2])
df.drop(range(0,10,2))
df.drop('class',axis=1)
df.drop(['class','age'],axis=1)
df['roll'] + 100
df['marks']=df['marks']+0.5
df
df['cgpa']=df['marks']/10+0.38
df

                                                   Q5.)Day3 - Data Preprocessing

import pandas as pd
df=pd.read_csv('student3.csv')
df
df['age'].mean()
df['marks '].mean()
df.columns=df.columns.str.strip()
df.columns
df
df['class'].value_counts()
df['class']=df['class'].str.upper()
df['class'].value_counts()
df['age'].value_counts()
df['name']=df['name'].str.title()
df
df.duplicated()
df[df.duplicated()]
df.duplicated().sum()
df.drop_duplicates(inplace=True)
df
df.info()
df.count()
df.isnull()
df.isnull().sum()
df.describe()
df.dropna()
df['age'].fillna(21)
df['marks'].fillna(df['marks'].mean())
df['class'].fillna(df['class'].mode()[0])
df.fillna(method='pad')
df.fillna(method='backfill')
df=pd.read_csv('student.csv')
df
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['name']=le.fit_transform(df['name'])
df
pd.get_dummies(df)
df= pd.read_csv('Social_Network_Ads.csv')
df
x=df[['Age','EstimatedSalary']]
import matplotlib.pyplot as plt
plt.scatter(x['Age'], x_scaled['EstimatedSalary'], c=df['Purchased'])
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_scaled=scaler.fit_transform(x)
x_scaled = pd.DataFrame(x_scaled, columns=x.columns)
x_scaled
x_scaled.describe()
x.std()
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled=scaler.fit_transform(x)
x_scaled = pd.DataFrame(x_scaled, columns=x.columns)
x_scaled.std()
plt.scatter(x['Age'], x_scaled['EstimatedSalary'], c=df['Purchased'])

                                                   Q6.)Decision Tree Algorithm
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target labels

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(max_depth=3, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Visualize the Decision Tree
plt.figure(figsize=(12,8))
tree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

# Display the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, target_names=iris.target_names))

                                                             Q7.) FashionCNN

# importing the libraries
import pandas as pd
import numpy as np
     
from skimage.io import imread
import matplotlib.pyplot as plt
%matplotlib inline
     
from sklearn.model_selection import train_test_split
     
from sklearn.metrics import accuracy_score
from tqdm import tqdm

import torch
from torch.autograd import Variable
#newly added for FashinMNIST Dataset
#####
from torch.utils.data import Dataset
from torchvision import datasets
#####
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD
     
import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
transform = transforms.Compose([transforms.ToTensor(),
  transforms.Normalize((0.5,), (0.5,))
])

training_data = datasets.FashionMNIST(root='contents/', download=True, transform=transform, train=True)
test_data = datasets.FashionMNIST(root='contents/', download=True, transform=transform, train=True)

train_loader = torch.utils.data.DataLoader(dataset = training_data,
                                           batch_size = 128,
                                           shuffle = True)

test_loader = torch.utils.data.DataLoader(dataset = test_data,
                                           batch_size = 128,
                                           shuffle = True)
     
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.imshow(training_data.data[i])
  plt.title('Ground Truth: {}'.format(training_data.classes[training_data.targets[i]]))
  plt.axis('off')

class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size= 5,stride = 1)
        self.conv2 = nn.Conv2d(32, 64, 5, 1)
        self.fc = nn.Linear(64*20*20, 47)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))

                                                    Q8.)K-Means Clustering

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df=pd.read_csv('Mall_Customers.csv')
df

x=df.iloc[:,3:]
x

plt.title('Data Distribution')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'],x['Spending Score (1-100)'])

from sklearn.cluster import KMeans

km=KMeans(n_clusters=3)
labels=km.fit_predict(x)
labels

km.inertia_

sse=[]
for k in range(1,16):
    km=KMeans(n_clusters=k)
    km.fit_predict(x)
    sse.append(km.inertia_)

sse

plt.plot(sse,color='red',marker='o',mfc='blue')

from kneed import KneeLocator
k1=KneeLocator(range(1,16),sse, curve='convex',direction='decreasing')
k1.elbow

km = KMeans(n_clusters=5)
lables=km.fit_predict(x)
lables

km.inertia_

plt.title('Data Distribution')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'],x['Spending Score (1-100)'],c=lables)

df[lables==0]

df[labels==0].max()

df[lables==0].min()

ZERO=df[lables==0]
ONE=df[lables==1]
TWO=df[lables==2]
THREE=df[lables==3]
FOUR=df[lables==4]
len(ZERO)

len(ONE)

len(TWO)

len(THREE)

len(FOUR)

                                                         Q9.)Linear Regression

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

data = pd.read_csv("D:\Subject\MLDL\Datasets\Simple linear regression1.01.csv")
data

data.describe()

y = data ['GPA']
# Similarly, our independent variable (x) is the SAT score
x1 = data ['SAT']

plt.scatter(x1,y)
# Name the axes
plt.xlabel('SAT', fontsize = 20)
plt.ylabel('GPA', fontsize = 20)
# Show the plot
plt.show()

# Add a constant. Essentially, we are adding a new column (equal in lenght to x), which consists only of 1s
x = sm.add_constant(x1)
# Fit the model, according to the OLS (ordinary least squares) method with a dependent variable y and an idependent x
results = sm.OLS(y,x).fit()
# Print a nice summary of the regression. That's one of the strong points of statsmodels -> the summaries
results.summary()

# Create a scatter plot
plt.scatter(x1,y)
# Define the regression equation, so we can plot it later
yhat = 0.0017*x1 + 0.275
# Plot the regression line against the independent variable (SAT)
fig = plt.plot(x1,yhat, lw=4, c='orange', label ='regression line')
# Label the axes
plt.xlabel('SAT', fontsize = 20)
plt.ylabel('GPA', fontsize = 20)
plt.show()

                                                     Q10.)Logistic Regression

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from scipy import stats
stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)

raw_data = pd.read_csv("D:\Subject\MLDL\Datasets\Example_bank_data.csv")
raw_data

data = raw_data.copy()
data = data.drop(['Unnamed: 0'], axis = 1)
# We use the map function to change any 'yes' values to 1 and 'no' values to 0. 
data['y'] = data['y'].map({'yes':1, 'no':0})
data

data.describe()

#Declare the dependent and independent variables

y = data['y']
x1 = data['duration']
#Simple Logistic Regression
x = sm.add_constant(x1)
reg_log = sm.Logit(y,x)
results_log = reg_log.fit()

results_log.summary()

plt.scatter(x1,y,color = 'C0')

# Don't forget to label your axes!
plt.xlabel('Duration', fontsize = 20)
plt.ylabel('Subscription', fontsize = 20)
plt.show()

np.set_printoptions(formatter={'float': lambda x: "{0:0.2f}".format(x)})
#np.set_printoptions(formatter=None)
results_log.predict()

np.array(data['y'])
results_log.pred_table()

cm_df = pd.DataFrame(results_log.pred_table())
cm_df.columns = ['Predicted 0','Predicted 1']
cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})
cm_df

cm = np.array(cm_df)
accuracy_train = (cm[0,0]+cm[1,1])/cm.sum()
accuracy_train

                                                       Q11.)Multiple Linear Regression

import numpy as np 
import pandas as pd 
import statsmodels.api as sm 
import matplotlib.pyplot as plt 
import seaborn as sns
sns.set() 
raw_data = pd.read_csv("D:\Subject\MLDL\Datasets\Dummies1.03.+.csv")
raw_data['Attendance'] = raw_data['Attendance'].map({'Yes':1,'No': 0}) 
raw_data.describe()

y = raw_data['GPA']
x1 = raw_data[['SAT','Attendance']]
x = sm.add_constant(x1)
results = sm.OLS(y,x).fit()
results.summary()

plt.scatter(raw_data['SAT'],y) 
yhat_no = 0.6439 + 0.0014*raw_data['SAT'] 
yhat_yes = 0.2226 + 0.0014*raw_data['SAT']

fig = plt.plot(raw_data['SAT'],yhat_no, lw=2, c='#006837')
fig = plt.plot(raw_data['SAT'],yhat_yes, lw=2, c='#a50026')
plt.xlabel('SAT', fontsize = 20)
plt.ylabel('GPA', fontsize = 20)
plt.show()

plt.scatter(raw_data['SAT'],y,c=raw_data['Attendance'],cmap='RdYlGn_r')
yhat_no = 0.6439 + 0.0014*raw_data['SAT']
yhat_yes = 0.2226 + 0.0014*raw_data['SAT']
fig = plt.plot(raw_data['SAT'],yhat_no, lw=2, c='#006837')
fig = plt.plot(raw_data['SAT'],yhat_yes, lw=2, c='#a50026') 
plt.xlabel('SAT', fontsize = 20) 
plt.ylabel('GPA', fontsize = 20)
plt.show()

plt.scatter(raw_data['SAT'],raw_data['GPA'],
c=raw_data['Attendance'],cmap='RdYlGn_r')
yhat_no = 0.6439 + 0.0014*raw_data['SAT']
yhat_yes =0.2226 + 0.0014*raw_data['SAT']
yhat = 0.0017*raw_data['SAT'] + 0.275

fig = plt.plot(raw_data['SAT'],yhat_no, lw=2, c='#006837', label ='regression line1')
fig = plt.plot(raw_data['SAT'],yhat_yes, lw=2, c='#a50026', label ='regression line2')
fig = plt.plot(raw_data['SAT'],yhat, lw=3, c='#4C72B0', label ='regression line')
plt.xlabel('SAT', fontsize = 20)
plt.ylabel('GPA', fontsize = 20)
plt.show()

new_data = pd.DataFrame({'const': 1,'SAT': [1700, 1921], 'Attendance': [0, 1]})
new_data = new_data[['const','SAT','Attendance']]
new_data
new_data.rename(index={0: 'Tom',1:'Jerry'})
predictions = results.predict(new_data)
predictions

predictionsdf = pd.DataFrame({'Predictions':predictions})
joined = new_data.join(predictionsdf)
joined.rename(index={0: 'Tom',1:'Jerry'})

                                            Q12.)Predicting Customer Churn Using Decision Trees

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn import tree
import matplotlib.pyplot as plt

# Generate a synthetic dataset
# For demonstration, let's create a synthetic dataset resembling customer churn
from sklearn.datasets import make_classification

# Generate a synthetic binary classification dataset
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2, n_classes=2, random_state=42)

# Convert the dataset into a pandas DataFrame
df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(X.shape[1])])
df['Churn'] = y

# Display the first few rows of the dataset
print("Sample Data:")
print(df.head())

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Churn']), df['Churn'], test_size=0.3, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(max_depth=5, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy * 100:.2f}%")

# Display the classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=["No Churn", "Churn"]))

# Visualize the Decision Tree
plt.figure(figsize=(20,10))
tree.plot_tree(clf, feature_names=df.columns[:-1], class_names=["No Churn", "Churn"], filled=True)
plt.show()

                                                     Q13.)Random forest model

#First, start with importing necessary Python packages âˆ’
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#Next, download the iris dataset from its weblink as follows âˆ’
path = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

#Next, we need to assign column names to the dataset as follows âˆ’
headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

#Now, we need to read dataset to pandas dataframe as follows âˆ’
dataset = pd.read_csv(path, names = headernames)
dataset.head()

#Data Preprocessing will be done with the help of following script lines.
X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, 4].values
X
Y

#Next, we will divide the data into train and test split. The followingcode will split the dataset into 70% training data and 30% of testing data âˆ’
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)
